{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Ignoring warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burn_Glucose Excel sheet imported as bg\n",
    "bg = pd.read_excel('Burn_Glucose 022020.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num = len(bg)\n",
    "col_num = len(bg.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features that are not as significant or having too many NaN values\n",
    "\n",
    "# Dropping features PER_CODE, Collection Date, Time Vitals, Time Labs\n",
    "bg_preprocessed = bg.drop(columns = ['PER_CODE', 'Collection Date', 'Time Vitals', 'Time Labs'], errors = 'ignore')\n",
    "\n",
    "# Dropping columns that contains 16% or more NaN entries\n",
    "drop_col = []\n",
    "for feature in bg_preprocessed.columns:\n",
    "    num_NaN = 0\n",
    "    \n",
    "    for entry in bg_preprocessed[feature]:\n",
    "        # if entry == NaN\n",
    "        if str(entry) == 'nan': \n",
    "            num_NaN += 1\n",
    "            \n",
    "    if num_NaN/row_num >= 0.165:\n",
    "        drop_col.append(feature)\n",
    "        \n",
    "bg_preprocessed = bg_preprocessed.drop(columns = drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows that contains NaN entries\n",
    "bg_preprocessed = bg_preprocessed.dropna(axis = 0)\n",
    "bg_preprocessed = bg_preprocessed.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing string entires for feature 'Vent' to binary values\n",
    "bg_preprocessed = bg_preprocessed.replace(to_replace = {'Yes':1, 'No':0}, value = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Mechanism -- Performance Evaluating Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for looping and averaging results\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def prediction_score(prediction, groundtruth):\n",
    "    precision = precision_score(groundtruth, prediction, average = None)\n",
    "    f1 = f1_score(groundtruth, prediction, average = None)\n",
    "    recall = recall_score(groundtruth, prediction, average = None)\n",
    "    return precision, f1, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# K-Neighbor Classifier:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_1 = KNeighborsClassifier(1)\n",
    "knn_2 = KNeighborsClassifier(2)\n",
    "knn_3 = KNeighborsClassifier(3)\n",
    "knn_4 = KNeighborsClassifier(4)\n",
    "knn_5 = KNeighborsClassifier(5)\n",
    "knn_6 = KNeighborsClassifier(6)\n",
    "knn_7 = KNeighborsClassifier(7)\n",
    "knn_8 = KNeighborsClassifier(8)\n",
    "knn_9 = KNeighborsClassifier(9)\n",
    "knn_10 = KNeighborsClassifier(10)\n",
    "\n",
    "# Naive Bayes Classifier - Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Naive Bayes Classifier - Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Multilayer Classifier hidden_layer_sizes = 150 yields the best prediction\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "\n",
    "# Ensemble - Random Forest n_estimators = 60 to 80 yields the best prediction\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 60)\n",
    "\n",
    "# Ensemble - AdaBoost n_estimators = 150 to 180 yields the best prediction\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(n_estimators = 175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Scaling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling options: 1. Standardization 2. Min-Max Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "stdScaler = StandardScaler()\n",
    "minMaxScaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using K-Fold method to evaluate performance\n",
    "from sklearn.model_selection import KFold\n",
    "from statistics import mean\n",
    "\n",
    "def CV_KFold(X, y, model, n_folds):\n",
    "    # input parameters X and y must be numpy arrays\n",
    "    kf = KFold(n_splits = n_folds, shuffle = True)\n",
    "    sep_0_precisions = []\n",
    "    sep_1_precisions = []\n",
    "    sep_0_f1 = []\n",
    "    sep_1_f1 = []\n",
    "    sep_0_recall = []\n",
    "    sep_1_recall = []\n",
    "    n_zeroDivisionError = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train,X_test,y_train,y_test = X[train_index],X[test_index],y[train_index],y[test_index]\n",
    "        y_prediction = model.fit(X_train,y_train).predict(X_test)\n",
    "        try: \n",
    "            ins_precision, ins_f1, ins_recall = prediction_score(y_prediction, y_test)\n",
    "        except:\n",
    "            n_zeroDivisionError = n_zeroDivisionError + 1\n",
    "            continue\n",
    "        sep_0_precisions.append(ins_precision[0])\n",
    "        sep_1_precisions.append(ins_precision[1])\n",
    "        sep_0_f1.append(ins_f1[0])\n",
    "        sep_1_f1.append(ins_f1[1])\n",
    "        sep_0_recall.append(ins_recall[0])\n",
    "        sep_1_recall.append(ins_recall[1])\n",
    "\n",
    "    avg_sep0_precision = mean(sep_0_precisions)\n",
    "    avg_sep1_precision = mean(sep_1_precisions)\n",
    "    avg_sep0_f1 = mean(sep_0_f1)\n",
    "    avg_sep1_f1 = mean(sep_1_f1)\n",
    "    avg_sep0_recall = mean(sep_0_recall)\n",
    "    avg_sep1_recall = mean(sep_1_recall)\n",
    "    print('Sepsis = 0 precision CV Average = ' + str(avg_sep0_precision))\n",
    "    print('Sepsis = 1 precision CV Average= ' + str(avg_sep1_precision))\n",
    "    print('Sepsis = 0 f1 CV Average = ' + str(avg_sep0_f1))\n",
    "    print('Sepsis = 1 f1 CV Average = ' + str(avg_sep1_f1))\n",
    "    print('Sepsis = 0 recall CV Average = ' + str(avg_sep0_recall))\n",
    "    print('Sepsis = 1 recall CV Average = ' + str(avg_sep1_recall))\n",
    "    if n_zeroDivisionError != 0:\n",
    "        print('Number of Zero Division Errors:', n_zeroDivisionError)\n",
    "        \n",
    "    return avg_sep0_precision, avg_sep1_precision, avg_sep0_f1, avg_sep1_f1, avg_sep0_recall, avg_sep1_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with current_repodata.json, will retry with next repodata source.\n",
      "Initial quick solve with frozen env failed.  Unfreezing env and trying again.\n",
      "Solving environment: failed with current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.10\n",
      "  latest version: 4.8.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/zhaohengtang/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - imbalanced-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.6.16          |           py37_0         148 KB  conda-forge\n",
      "    conda-4.8.2                |           py37_0         3.0 MB  conda-forge\n",
      "    imbalanced-learn-0.5.0     |             py_0          98 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  imbalanced-learn   conda-forge/noarch::imbalanced-learn-0.5.0-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                      pkgs/main::conda-4.7.10-py37_0 --> conda-forge::conda-4.8.2-py37_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                         pkgs/main --> conda-forge\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "imbalanced-learn-0.5 | 98 KB     | ##################################### | 100% \n",
      "certifi-2019.6.16    | 148 KB    | ##################################### | 100% \n",
      "conda-4.8.2          | 3.0 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = bg_preprocessed.columns.difference(['Sepsis'])\n",
    "X = bg_preprocessed[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4477\n",
       "1     736\n",
       "Name: Sepsis, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bg_preprocessed.Sepsis\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4477\n",
       "0    4477\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE (Synthetic Minority Oversampling TEchnique)\n",
    "smote = SMOTE()\n",
    "\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "y_smote = pd.Series(y_smote)\n",
    "\n",
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Some Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sepsis = 0 precision CV Average = 0.8364066265064215\n",
      "Sepsis = 1 precision CV Average= 0.8745470253765772\n",
      "Sepsis = 0 f1 CV Average = 0.8582924805299925\n",
      "Sepsis = 1 f1 CV Average = 0.849972698393425\n",
      "Sepsis = 0 recall CV Average = 0.8819153316421197\n",
      "Sepsis = 1 recall CV Average = 0.8272972540079344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_smote_scaled = stdScaler.fit_transform(X_smote)\n",
    "\n",
    "avg_sep0_precision, avg_sep1_precision, avg_sep0_f1, avg_sep1_f1, avg_sep0_recall, avg_sep1_recall = CV_KFold(X_smote_scaled, y_smote, adaboost, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
